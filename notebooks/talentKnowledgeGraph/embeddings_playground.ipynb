{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo list:\n",
    "Lets first get the embeddings done, then work on a bit metadata.\n",
    "- update projected_embedding_ggvec_top40_4d_order2.json (author embeddings).\n",
    "    - consider neiborhoods. neibors are thoese who have ever collaborated.\n",
    "    - author embeddings are aggreated from paper embeddings.\n",
    "    - we can do 30K authors.\n",
    "    \n",
    "- update processed-metadata.csv, row id is author id, with author info.\n",
    "    - (Author info should be enriched, now we can have Number of pubmed papers.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "paper_author = pd.read_csv('/data/jx4237data/TKG/TKG_JCDL/Bridge2AI_2m/paper_author.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_author40k = pd.read_csv('/data/jx4237data/TKG/TKG_JCDL/Bridge2AI_10k/paper_author.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Au_Order</th>\n",
       "      <th>AID</th>\n",
       "      <th>AuthorNum</th>\n",
       "      <th>PubYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946486</td>\n",
       "      <td>1</td>\n",
       "      <td>6494255</td>\n",
       "      <td>3</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946486</td>\n",
       "      <td>2</td>\n",
       "      <td>100000086</td>\n",
       "      <td>3</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946486</td>\n",
       "      <td>3</td>\n",
       "      <td>7066054</td>\n",
       "      <td>3</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1077385</td>\n",
       "      <td>1</td>\n",
       "      <td>1644286</td>\n",
       "      <td>2</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1077385</td>\n",
       "      <td>2</td>\n",
       "      <td>100000021</td>\n",
       "      <td>2</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95417</th>\n",
       "      <td>38088622</td>\n",
       "      <td>4</td>\n",
       "      <td>26198780</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95418</th>\n",
       "      <td>38088622</td>\n",
       "      <td>5</td>\n",
       "      <td>26198781</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95419</th>\n",
       "      <td>38088622</td>\n",
       "      <td>6</td>\n",
       "      <td>11949134</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95420</th>\n",
       "      <td>38088622</td>\n",
       "      <td>7</td>\n",
       "      <td>26198782</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95421</th>\n",
       "      <td>38088622</td>\n",
       "      <td>8</td>\n",
       "      <td>1098</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95422 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PMID  Au_Order        AID  AuthorNum  PubYear\n",
       "0        946486         1    6494255          3     1976\n",
       "1        946486         2  100000086          3     1976\n",
       "2        946486         3    7066054          3     1976\n",
       "3       1077385         1    1644286          2     1976\n",
       "4       1077385         2  100000021          2     1976\n",
       "...         ...       ...        ...        ...      ...\n",
       "95417  38088622         4   26198780          8     2023\n",
       "95418  38088622         5   26198781          8     2023\n",
       "95419  38088622         6   11949134          8     2023\n",
       "95420  38088622         7   26198782          8     2023\n",
       "95421  38088622         8       1098          8     2023\n",
       "\n",
       "[95422 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter paper_author by only keep the rows that have AID in paper_author40k\n",
    "paper_author = paper_author[paper_author['AID'].isin(paper_author40k['AID'])]\n",
    "paper_author.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read /data/jx4237data/Graph-CoT/Pipeline/updated_data/tkg_embeddings_all.npz, this is a npz file\n",
    "import numpy as np\n",
    "# read the .npz file\n",
    "tkg_embeddings_all = np.load('/data/jx4237data/Graph-CoT/Pipeline/updated_data/tkg_embeddings_all.npz')\n",
    "\n",
    "# access the data from the .npz file\n",
    "embeddings = tkg_embeddings_all['embeddings']\n",
    "ids = tkg_embeddings_all['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many PMIDs in paper_author['PMID'] (int) are in ids (str), convert both into str\n",
    "# list pmids in paper_author['PMID'] but not in ids\n",
    "pmids = paper_author['PMID'].astype(str)\n",
    "ids = [str(i) for i in ids]\n",
    "pmids_not_in_ids = list(set(pmids) - set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read /data/jx4237data/projects/get_methods/pubmed24_title_abstract.csv.gz all cols are strings \\\n",
    "# only keep the rows with PMID in pmids_not_in_ids, use tqdm to show progress\n",
    "# Define the file path\n",
    "file_path = '/data/jx4237data/projects/get_methods/pubmed24_title_abstract.csv.gz'\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 100000\n",
    "\n",
    "# Create an empty list to store the filtered chunks\n",
    "filtered_chunks = []\n",
    "\n",
    "# Iterate over the file in chunks\n",
    "for chunk in tqdm(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "    # Filter the chunk based on PMID\n",
    "    filtered_chunk = chunk[chunk['PMID'].astype(str).isin(pmids_not_in_ids)]\n",
    "    \n",
    "    # Append the filtered chunk to the list\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "    \n",
    "# Concatenate the filtered chunks into a single dataframe\n",
    "filtered_df = pd.concat(filtered_chunks)\n",
    "\n",
    "# Free memory by deleting the chunks\n",
    "del filtered_chunks\n",
    "\n",
    "# Print the filtered dataframe\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the TOKENIZERS_PARALLELISM environment variable to false to avoid warnings and potential deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set visible GPUs before importing PyTorch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_aug2023refresh_base')\n",
    "\n",
    "# Load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\n",
    "model.load_adapter(\"allenai/specter2_aug2023refresh\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
    "\n",
    "# Prepare new documents and IDs\n",
    "new_docs = []\n",
    "new_ids = []\n",
    "for index, row in filtered_df.iterrows():\n",
    "    title = row['Title']\n",
    "    abstract = row['Abstract']\n",
    "    pmid = row['PMID']\n",
    "    new_docs.append(str(title) + tokenizer.sep_token + str(abstract))\n",
    "    new_ids.append(pmid)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 100  # Define an appropriate batch size\n",
    "all_embeddings = []\n",
    "all_ids = []\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, docs, ids):\n",
    "        self.docs = docs\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.docs[idx], self.ids[idx]\n",
    "\n",
    "dataset = DocumentDataset(new_docs, new_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Function to process a single batch\n",
    "def process_batch(batch):\n",
    "    docs, ids = batch\n",
    "    inputs = tokenizer(list(docs), padding=True, truncation=True,\n",
    "                       return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "    inputs = inputs.to(device)  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        output = model(**inputs)\n",
    "    batch_embeddings = output.last_hidden_state[:, 0, :].cpu()  # Move embeddings back to CPU\n",
    "    return batch_embeddings, ids\n",
    "\n",
    "# Process the dataset in batches\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_embeddings, batch_ids = process_batch(batch)\n",
    "    \n",
    "    all_embeddings.append(batch_embeddings)\n",
    "    all_ids.extend(batch_ids)\n",
    "\n",
    "new_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
    "new_ids = all_ids\n",
    "\n",
    "print(\"Embeddings have been generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids_str = [str(tensor.tolist()) for tensor in new_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_ids_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings_dict = {id_: embedding for id_, embedding in zip(new_ids_str, new_embeddings)}\n",
    "# store this dict in a comressed file\n",
    "import numpy as np\n",
    "np.savez_compressed('/data/jx4237data/TKG/TKG_JCDL/Bridge2AI_10k/tkg_updated_360k_embeddings_all.npz', embeddings=new_embeddings, ids=new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have new_embeddings, new_ids, ids, embeddings, we need to concatenate them. \n",
    "# and get a dict with key as PMID and value as embedding (for both embeddings and new_embeddings, get one embedding for each PMID)\n",
    "# Create a dictionary with PMID as key and embedding as value\n",
    "embeddings_dict = {id_: embedding for id_, embedding in zip(ids, embeddings)}\n",
    "# new_embeddings_dict = {str(id_): embedding for id_, embedding in zip(new_ids, new_embeddings)}\n",
    "combined_embeddings_dict = {**embeddings_dict, **new_embeddings_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now based on the combined_embeddings_dict, (it represent each paper's embedding),\n",
    "# we need to get the embedding for each author, by a weighted sum of the embeddings of the papers that the author has written\n",
    "# paper_author has PMID, AID, Au_Order, AuthorNum\n",
    "# our weighted strategy is like this:\n",
    "# for author aggregation:\n",
    "# author rank - weights\n",
    "# 1st - 1\n",
    "# last - 1\n",
    "# 2nd - 1/2\n",
    "# 3rd - 1/3\n",
    "# 4th - 1/4\n",
    "# ... no less than 1/10\n",
    "# Write the code:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_author_embeddings(combined_embeddings_dict, paper_author):\n",
    "    author_embeddings = {}\n",
    "    author_weights = {}\n",
    "\n",
    "    for index, entry in tqdm(paper_author.iterrows(), total=paper_author.shape[0], desc=\"Processing authors\"):\n",
    "        pmid = str(entry['PMID'])\n",
    "        aid = entry['AID']\n",
    "        au_order = entry['Au_Order']\n",
    "        author_num = entry['AuthorNum']\n",
    "\n",
    "        # Get the embedding for the paper\n",
    "        paper_embedding = combined_embeddings_dict[pmid]\n",
    "\n",
    "        # Calculate the weight based on the author's order\n",
    "        if au_order == 1 or au_order == author_num:\n",
    "            weight = 1\n",
    "        else:\n",
    "            weight = max(1 / au_order, 1 / 10)\n",
    "\n",
    "        # Initialize the author's embedding and weight if not already done\n",
    "        if aid not in author_embeddings:\n",
    "            author_embeddings[aid] = np.zeros_like(paper_embedding)\n",
    "            author_weights[aid] = 0\n",
    "\n",
    "        # Update the author's cumulative embedding and weight\n",
    "        author_embeddings[aid] += weight * paper_embedding\n",
    "        author_weights[aid] += weight\n",
    "\n",
    "    # Normalize the embeddings by the sum of weights\n",
    "    for aid in author_embeddings:\n",
    "        author_embeddings[aid] /= author_weights[aid]\n",
    "\n",
    "    return author_embeddings\n",
    "\n",
    "author_embeddings = get_author_embeddings(combined_embeddings_dict, paper_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# Save the dictionary to a .gz file\n",
    "with gzip.open('author_embeddings.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(author_embeddings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load the dictionary from the .gz file\n",
    "with gzip.open('author_embeddings.pkl.gz', 'rb') as f:\n",
    "    author_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the .gz file\n",
    "with gzip.open('author_embeddings.pkl.gz', 'rb') as f:\n",
    "    author_embeddings = pickle.load(f)\n",
    "\n",
    "len(author_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now based on the paper_author, let find neibor authors for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_author is a dataframe, has PMID, AID, Au_Order, AuthorNum, \n",
    "# now based on the paper_author, let find neibor authors for each author.\n",
    "# Define a function to get the neighboring authors for each author\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_neighbors(paper_author):\n",
    "    # Initialize a dictionary to store neighbors for each author\n",
    "    author_neighbors = defaultdict(set)\n",
    "\n",
    "    # Group the dataframe by PMID\n",
    "    grouped = paper_author.groupby('PMID')\n",
    "\n",
    "    # Iterate through each group (each paper)\n",
    "    for pmid, group in grouped:\n",
    "        # Get the list of authors for this paper\n",
    "        authors = group['AID'].tolist()\n",
    "\n",
    "        # For each author, add the other authors as neighbors\n",
    "        for author in authors:\n",
    "            neighbors = set(authors) - {author}\n",
    "            author_neighbors[author].update(neighbors)\n",
    "\n",
    "    # Convert sets to lists for the final output\n",
    "    author_neighbors = {author: list(neighbors) for author, neighbors in author_neighbors.items()}\n",
    "\n",
    "    return author_neighbors\n",
    "\n",
    "author_neighbors = find_neighbors(paper_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_author_neighbors = {}\n",
    "for author,neighbors in author_neighbors.items():\n",
    "    if 0 in neighbors:\n",
    "        neighbors.remove(0)\n",
    "    new_author_neighbors[author] = neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a .gz file\n",
    "with gzip.open('author_collaborators.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(new_author_neighbors, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load the dictionary from the .gz file\n",
    "with gzip.open('author_collaborators.pkl.gz', 'rb') as f:\n",
    "    author_neighbors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read /data/jx4237data/TKG/TKG_JCDL/Bridge2AI_10k/Authors.csv.gz\n",
    "authors = pd.read_csv('/data/jx4237data/TKG/TKG_JCDL/Bridge2AI_10k/Authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = authors[['AID', 'FullName', 'BeginYear', 'PaperNum','CM4AI','RecentYear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>FullName</th>\n",
       "      <th>BeginYear</th>\n",
       "      <th>PaperNum</th>\n",
       "      <th>CM4AI</th>\n",
       "      <th>RecentYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>Baba Inusa</td>\n",
       "      <td>2005</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>878</td>\n",
       "      <td>Rutger Schutten</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1098</td>\n",
       "      <td>Vardit Ravitsky (CM4AI)</td>\n",
       "      <td>2002</td>\n",
       "      <td>192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1352</td>\n",
       "      <td>Ursula Grohmann</td>\n",
       "      <td>1988</td>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1592</td>\n",
       "      <td>Sharon R Lewin</td>\n",
       "      <td>1993</td>\n",
       "      <td>373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44678</th>\n",
       "      <td>843771129831</td>\n",
       "      <td>Jillian Mohan (CM4AI)</td>\n",
       "      <td>2012</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44679</th>\n",
       "      <td>83284832748327</td>\n",
       "      <td>Jiawei Xu (CM4AI)</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44680</th>\n",
       "      <td>873811147837</td>\n",
       "      <td>Swathi Thaker (CM4AI)</td>\n",
       "      <td>2005</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44681</th>\n",
       "      <td>23553111988</td>\n",
       "      <td>Xiaoyu Zhao (CM4AI)</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44682</th>\n",
       "      <td>89432444320001</td>\n",
       "      <td>Zhandos Sembay (CM4AI)</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AID                 FullName  BeginYear  PaperNum  CM4AI  \\\n",
       "0                 225               Baba Inusa       2005       102    NaN   \n",
       "1                 878          Rutger Schutten       2017         5    NaN   \n",
       "3                1098  Vardit Ravitsky (CM4AI)       2002       192    1.0   \n",
       "4                1352          Ursula Grohmann       1988       158    NaN   \n",
       "5                1592           Sharon R Lewin       1993       373    NaN   \n",
       "...               ...                      ...        ...       ...    ...   \n",
       "44678    843771129831    Jillian Mohan (CM4AI)       2012        37    1.0   \n",
       "44679  83284832748327        Jiawei Xu (CM4AI)       2021         5    1.0   \n",
       "44680    873811147837    Swathi Thaker (CM4AI)       2005        23    1.0   \n",
       "44681     23553111988      Xiaoyu Zhao (CM4AI)       2010        11    1.0   \n",
       "44682  89432444320001   Zhandos Sembay (CM4AI)       2021         4    1.0   \n",
       "\n",
       "       RecentYear  \n",
       "0            2024  \n",
       "1            2022  \n",
       "3            2023  \n",
       "4            2023  \n",
       "5            2023  \n",
       "...           ...  \n",
       "44678        2024  \n",
       "44679        2024  \n",
       "44680        2024  \n",
       "44681        2024  \n",
       "44682        2024  \n",
       "\n",
       "[28400 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = authors[(authors['PaperNum'] > 2) & (authors['RecentYear'] > 2020)]\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15074/3364539963.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  authors['BeginYear'] = authors['BeginYear'].astype(int)\n",
      "/tmp/ipykernel_15074/3364539963.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  authors['PaperNum'] = authors['PaperNum'].astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AID</th>\n",
       "      <th>FullName</th>\n",
       "      <th>BeginYear</th>\n",
       "      <th>PaperNum</th>\n",
       "      <th>CM4AI</th>\n",
       "      <th>RecentYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>Baba Inusa</td>\n",
       "      <td>2005</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>878</td>\n",
       "      <td>Rutger Schutten</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1098</td>\n",
       "      <td>Vardit Ravitsky (CM4AI)</td>\n",
       "      <td>2002</td>\n",
       "      <td>192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1352</td>\n",
       "      <td>Ursula Grohmann</td>\n",
       "      <td>1988</td>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1592</td>\n",
       "      <td>Sharon R Lewin</td>\n",
       "      <td>1993</td>\n",
       "      <td>373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44678</th>\n",
       "      <td>843771129831</td>\n",
       "      <td>Jillian Mohan (CM4AI)</td>\n",
       "      <td>2012</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44679</th>\n",
       "      <td>83284832748327</td>\n",
       "      <td>Jiawei Xu (CM4AI)</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44680</th>\n",
       "      <td>873811147837</td>\n",
       "      <td>Swathi Thaker (CM4AI)</td>\n",
       "      <td>2005</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44681</th>\n",
       "      <td>23553111988</td>\n",
       "      <td>Xiaoyu Zhao (CM4AI)</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44682</th>\n",
       "      <td>89432444320001</td>\n",
       "      <td>Zhandos Sembay (CM4AI)</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AID                 FullName  BeginYear  PaperNum  CM4AI  \\\n",
       "0                 225               Baba Inusa       2005       102    NaN   \n",
       "1                 878          Rutger Schutten       2017         5    NaN   \n",
       "3                1098  Vardit Ravitsky (CM4AI)       2002       192    1.0   \n",
       "4                1352          Ursula Grohmann       1988       158    NaN   \n",
       "5                1592           Sharon R Lewin       1993       373    NaN   \n",
       "...               ...                      ...        ...       ...    ...   \n",
       "44678    843771129831    Jillian Mohan (CM4AI)       2012        37    1.0   \n",
       "44679  83284832748327        Jiawei Xu (CM4AI)       2021         5    1.0   \n",
       "44680    873811147837    Swathi Thaker (CM4AI)       2005        23    1.0   \n",
       "44681     23553111988      Xiaoyu Zhao (CM4AI)       2010        11    1.0   \n",
       "44682  89432444320001   Zhandos Sembay (CM4AI)       2021         4    1.0   \n",
       "\n",
       "       RecentYear  \n",
       "0            2024  \n",
       "1            2022  \n",
       "3            2023  \n",
       "4            2023  \n",
       "5            2023  \n",
       "...           ...  \n",
       "44678        2024  \n",
       "44679        2024  \n",
       "44680        2024  \n",
       "44681        2024  \n",
       "44682        2024  \n",
       "\n",
       "[28400 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authors['BeginYear'] and authors['PaperNum'] are float64, convert them to int\n",
    "authors['BeginYear'] = authors['BeginYear'].astype(int)\n",
    "authors['PaperNum'] = authors['PaperNum'].astype(int)\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_reset = authors.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_reset.rename(columns={'AID': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_reset['Index'] = authors_reset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_reset.to_csv('/data/jx4237data/projects/web_page/talentKnowledgeGraph/work/data/processed_authors_more_than_single_recent_pub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_reset = pd.read_csv('/data/jx4237data/projects/web_page/talentKnowledgeGraph/work/data/processed_authors_more_than_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv('/data/jx4237data/projects/web_page/talentKnowledgeGraph/work/data/processed-metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9432.00\n",
       "mean        0.23\n",
       "std         0.17\n",
       "min         0.14\n",
       "25%         0.15\n",
       "50%         0.16\n",
       "75%         0.22\n",
       "max         2.48\n",
       "0.6         0.18\n",
       "0.8         0.25\n",
       "0.9         0.37\n",
       "0.95        0.52\n",
       "Name: rating_count, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the transformed rating count\n",
    "transformed_rating_count = ((anime['rating_count']**0.72) / 9000 + 0.14)\n",
    "\n",
    "# Describe the transformed rating count\n",
    "description = transformed_rating_count.describe()\n",
    "\n",
    "# Calculate additional percentiles\n",
    "percentiles = transformed_rating_count.quantile([0.60, 0.80, 0.90, 0.95])\n",
    "\n",
    "# Combine the description and percentiles\n",
    "description = pd.concat([description, percentiles])\n",
    "\n",
    "# Format the description to 2 decimal places\n",
    "formatted_description = description.apply(lambda x: format(x, '.2f'))\n",
    "\n",
    "formatted_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    34638.00\n",
       "mean         3.43\n",
       "std          3.29\n",
       "min          0.41\n",
       "25%          1.20\n",
       "50%          2.31\n",
       "75%          4.53\n",
       "max         39.63\n",
       "0.6          3.00\n",
       "0.8          5.25\n",
       "0.9          7.64\n",
       "0.95         9.96\n",
       "Name: PaperNum, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the transformed rating count\n",
    "transformed_rating_count = ((authors_reset['PaperNum'])**.7 / 6 + 0.14)\n",
    "\n",
    "# Describe the transformed rating count\n",
    "description = transformed_rating_count.describe()\n",
    "\n",
    "# Calculate additional percentiles\n",
    "percentiles = transformed_rating_count.quantile([0.60, 0.80, 0.90, 0.95])\n",
    "\n",
    "# Combine the description and percentiles\n",
    "description = pd.concat([description, percentiles])\n",
    "\n",
    "# Format the description to 2 decimal places\n",
    "formatted_description = description.apply(lambda x: format(x, '.2f'))\n",
    "\n",
    "formatted_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    34638.00\n",
       "mean        88.91\n",
       "std        135.61\n",
       "min          2.00\n",
       "25%         14.00\n",
       "50%         39.00\n",
       "75%        107.00\n",
       "max       2468.00\n",
       "0.6         58.00\n",
       "0.8        133.00\n",
       "0.9        230.00\n",
       "0.95       338.00\n",
       "Name: PaperNum, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the transformed rating count\n",
    "transformed_rating_count = authors_reset['PaperNum']\n",
    "\n",
    "# Describe the transformed rating count\n",
    "description = transformed_rating_count.describe()\n",
    "\n",
    "# Calculate additional percentiles\n",
    "percentiles = transformed_rating_count.quantile([0.60, 0.80, 0.90, 0.95])\n",
    "\n",
    "# Combine the description and percentiles\n",
    "description = pd.concat([description, percentiles])\n",
    "\n",
    "# Format the description to 2 decimal places\n",
    "formatted_description = description.apply(lambda x: format(x, '.2f'))\n",
    "\n",
    "formatted_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    44673.00\n",
       "mean      2005.32\n",
       "std         13.57\n",
       "min       1804.00\n",
       "25%       1997.00\n",
       "50%       2007.00\n",
       "75%       2016.00\n",
       "max       2024.00\n",
       "0.02      1974.00\n",
       "0.05      1980.00\n",
       "0.1       1986.00\n",
       "0.15      1991.00\n",
       "0.2       1994.00\n",
       "0.6       2011.00\n",
       "0.8       2019.00\n",
       "0.9       2022.00\n",
       "0.95      2023.00\n",
       "Name: BeginYear, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the transformed rating count\n",
    "transformed_rating_count = (authors['BeginYear'])\n",
    "\n",
    "# Describe the transformed rating count\n",
    "description = transformed_rating_count.describe()\n",
    "\n",
    "# Calculate additional percentiles\n",
    "percentiles = transformed_rating_count.quantile([0.02, 0.05, .1,.15, .2 , 0.60, 0.80, 0.90, 0.95])\n",
    "\n",
    "# Combine the description and percentiles\n",
    "description = pd.concat([description, percentiles])\n",
    "\n",
    "# Format the description to 2 decimal places\n",
    "formatted_description = description.apply(lambda x: format(x, '.2f'))\n",
    "\n",
    "formatted_description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
