{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt update\n",
    "# !sudo apt -y install build-essential vim\n",
    "# !pip3 install networkx node2vec python-Levenshtein nodevectors emblaze pymde\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'title_english', 'title_japanese', 'title_synonyms',\n",
       "       'aired_from_year', 'rating_count', 'average_rating', 'image_url',\n",
       "       'type', 'source', 'episodes', 'status', 'airing', 'aired_string',\n",
       "       'aired', 'duration', 'rating', 'score', 'rank', 'popularity', 'members',\n",
       "       'favorites', 'background', 'premiered', 'broadcast', 'related_anime',\n",
       "       'producer', 'licensor', 'studio', 'genre', 'opening_theme',\n",
       "       'ending_theme', 'aired_from_year_string'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_animes_df = pd.read_csv('/data/jx4237data/projects/web_page/sprout-main/work/data/processed-metadata.csv')\n",
    "all_animes_df['average_rating'] = all_animes_df['score']\n",
    "all_animes_df.columns\n",
    "\n",
    "new_order = [\n",
    "    'id', 'title', 'title_english', 'title_japanese', 'title_synonyms',\n",
    "    'aired_from_year', 'rating_count', 'average_rating', 'image_url', 'type', 'source', 'episodes', 'status', 'airing',\n",
    "    'aired_string', 'aired', 'duration', 'rating', 'score', \n",
    "    'rank', 'popularity', 'members', 'favorites', 'background', 'premiered',\n",
    "    'broadcast', 'related_anime', 'producer', 'licensor', 'studio', 'genre',\n",
    "    'opening_theme', 'ending_theme', 'aired_from_year_string'\n",
    "]\n",
    "# Reindex the DataFrame with the new order of columns\n",
    "all_animes_df = all_animes_df.reindex(columns=new_order)\n",
    "all_animes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_animes_df.to_csv('/data/jx4237data/projects/web_page/sprout-main/work/data/processed-metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2012\n",
       "1       2007\n",
       "2       2008\n",
       "3       2002\n",
       "4       2012\n",
       "        ... \n",
       "9427    2018\n",
       "9428    2015\n",
       "9429    2017\n",
       "9430    1987\n",
       "9431    2010\n",
       "Name: aired_from_year, Length: 9432, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_animes = []\n",
    "anime_title_by_id = {}\n",
    "anime_id_by_title = {}\n",
    "anime_ix_by_id = {}\n",
    "anime_by_id = {}\n",
    "\n",
    "i = 0\n",
    "for anime in all_animes_df.itertuples(index=False):\n",
    "    all_animes.append(anime)\n",
    "    anime_title_by_id[anime.id] = anime.title\n",
    "    anime_id_by_title[anime.title] = anime.id\n",
    "    anime_ix_by_id[anime.id] = i\n",
    "    anime_by_id[anime.id] = anime\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv('/data/jx4237data/projects/web_page/sprout-main/work/data/collected_animelists.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining ratings: 39126493\n",
      "   username  anime_id  my_score  anime_ix\n",
      "0  karthiga        21       9.1        34\n",
      "1  karthiga        59       3.3        15\n",
      "2  karthiga        74       3.3        21\n",
      "3  karthiga       120       3.3        32\n",
      "4  karthiga       178       3.3         6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only retain the \"username\", \"anime_id\", and \"my_score\" columns\n",
    "ratings = all_ratings[['username', 'anime_id', 'my_score']]\n",
    "# only retain rows where \"my_score\" is not null and greater than or equal to 6\n",
    "ratings = ratings[ratings['my_score'].notnull()]\n",
    "ratings = ratings[ratings['my_score'] > 0]\n",
    "ratings = ratings[ratings['anime_id'].isin(anime_title_by_id.keys())]\n",
    "# change to ix\n",
    "ratings['anime_ix'] = ratings['anime_id'].apply(lambda x: anime_ix_by_id[x])\n",
    "\n",
    "def scale_rating(rating: int) -> float:\n",
    "    if rating == 10:\n",
    "        return 10.1\n",
    "    if rating == 9:\n",
    "        return 9.1\n",
    "    if rating == 8:\n",
    "        return 6.5\n",
    "    if rating == 7:\n",
    "        return 3.3\n",
    "    if rating == 6:\n",
    "        return 0.5\n",
    "    if rating == 5:\n",
    "        return -1.5\n",
    "    if rating == 4:\n",
    "        return -4.5\n",
    "    if rating == 3:\n",
    "        return -7.5\n",
    "    if rating == 2:\n",
    "        return -10.5\n",
    "    if rating == 1:\n",
    "        return -13.5\n",
    "    raise ValueError(\"Invalid rating: {}\".format(rating))\n",
    "\n",
    "# scale ratings from score to our custom scale\n",
    "ratings['my_score'] = ratings['my_score'].apply(scale_rating)\n",
    "# only keep high scores\n",
    "ratings = ratings[ratings['my_score'] > 0]\n",
    "\n",
    "print(\"Remaining ratings:\", ratings.shape[0])\n",
    "print(ratings.head())\n",
    "\n",
    "# collect python garbage\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 39126493 ratings out of 39126493 total\n"
     ]
    }
   ],
   "source": [
    "# ratings_subset = ratings.sample(n=5000000)\n",
    "# ratings_subset = ratings[:5000000]\n",
    "ratings_subset = ratings\n",
    "\n",
    "# Retain only ratings of anime that have >=100 ratings\n",
    "ratings_subset = ratings_subset[ratings['anime_id'].isin(anime_title_by_id.keys())]\n",
    "print(f\"Retained {ratings_subset.shape[0]} ratings out of {ratings.shape[0]} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: 9432\n",
      "ixs: 9432\n"
     ]
    }
   ],
   "source": [
    "all_ids = set(anime_ix_by_id.keys())\n",
    "all_ixs = set(anime_ix_by_id.values())\n",
    "\n",
    "print(f\"ids: {len(all_ids)}\")\n",
    "print(f\"ixs: {len(all_ixs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 1000000 rows\n",
      "Processed 2000000 rows\n",
      "Processed 3000000 rows\n",
      "Processed 4000000 rows\n",
      "Processed 5000000 rows\n",
      "Processed 6000000 rows\n",
      "Processed 7000000 rows\n",
      "Processed 8000000 rows\n",
      "Processed 9000000 rows\n",
      "Processed 10000000 rows\n",
      "Processed 11000000 rows\n",
      "Processed 12000000 rows\n",
      "Processed 13000000 rows\n",
      "Processed 14000000 rows\n",
      "Processed 15000000 rows\n",
      "Processed 16000000 rows\n",
      "Processed 17000000 rows\n",
      "Processed 18000000 rows\n",
      "Processed 19000000 rows\n",
      "Processed 20000000 rows\n",
      "Processed 21000000 rows\n",
      "Processed 22000000 rows\n",
      "Processed 23000000 rows\n",
      "Processed 24000000 rows\n",
      "Processed 25000000 rows\n",
      "Processed 26000000 rows\n",
      "Processed 27000000 rows\n",
      "Processed 28000000 rows\n",
      "Processed 29000000 rows\n",
      "Processed 30000000 rows\n",
      "Processed 31000000 rows\n",
      "Processed 32000000 rows\n",
      "Processed 33000000 rows\n",
      "Processed 34000000 rows\n",
      "Processed 35000000 rows\n",
      "Processed 36000000 rows\n",
      "Processed 37000000 rows\n",
      "Processed 38000000 rows\n",
      "Processed 39000000 rows\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ratings_by_username = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "for row in ratings_subset.itertuples():\n",
    "    if i % 1000000 == 0:\n",
    "        print(f\"Processed {i} rows\")\n",
    "    i += 1\n",
    "\n",
    "    rating = row.my_score\n",
    "    if rating < 0:\n",
    "        continue\n",
    "    anime_ix = anime_ix_by_id[row.anime_id]\n",
    "    ratings_by_username[row.username].append((anime_ix, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each user's ratings, deduplicate.\n",
    "processed_rows = 0\n",
    "for username in ratings_by_username.keys():\n",
    "    before_len = len(ratings_by_username[username])\n",
    "    ratings_by_username[username] = list(set(ratings_by_username[username]))\n",
    "    after_len = len(ratings_by_username[username])\n",
    "    if before_len != after_len:\n",
    "        print(f\"Removed {before_len - after_len} duplicate ratings for user {username}\")\n",
    "    processed_rows += 1\n",
    "    if processed_rows % 500000 == 0:\n",
    "        print(f\"Processed {processed_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings = [(np.asarray([anime_ix for anime_ix, rating in ratings]), np.asarray([rating for anime_ix, rating in ratings])) for ratings in ratings_by_username.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jx4237data/python_envs/sprout-env/lib/python3.10/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users to process: 270944\n",
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function compute_cooccurrence_matrix, \n",
      "/tmp/ipykernel_2488519/1771784096.py (5)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function compute_cooccurrence_matrix, /tmp/ipykernel_2488519/1771784096.py (5) \n",
      "---------------------------------------------------------------------------------------------------------|loop #ID\n",
      "@njit(parallel=True, nopython=True)                                                                      | \n",
      "def compute_cooccurrence_matrix(anime_count: int, ratings: list[(np.array, np.array)]) -> np.ndarray:    | \n",
      "    cooccurrence_matrix = np.zeros((anime_count, anime_count))-------------------------------------------| #0\n",
      "                                                                                                         | \n",
      "    for anime_indices, ratings in ratings:                                                               | \n",
      "        rating_count_for_user = len(anime_indices)                                                       | \n",
      "                                                                                                         | \n",
      "        for i in prange(rating_count_for_user):----------------------------------------------------------| #1\n",
      "            for j in range(rating_count_for_user):                                                       | \n",
      "                if i == j:                                                                               | \n",
      "                    continue                                                                             | \n",
      "                                                                                                         | \n",
      "                cooccurrence_matrix[anime_indices[i], anime_indices[j]] += ratings[i] * ratings[j]       | \n",
      "                                                                                                         | \n",
      "    return cooccurrence_matrix                                                                           | \n",
      "--------------------------------- Fusing loops ---------------------------------\n",
      "Attempting fusion of parallel loops (combines loops with similar properties)...\n",
      "----------------------------- Before Optimisation ------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel structure is already optimal.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "---------------------------Loop invariant code motion---------------------------\n",
      "Allocation hoisting:\n",
      "No allocation hoisting found\n",
      "\n",
      "Instruction hoisting:\n",
      "loop #0:\n",
      "  Has the following hoisted:\n",
      "    $expr_out_var.15 = const(float64, 0.0)\n",
      "  Failed to hoist the following:\n",
      "    dependency: $parfor_index_tuple_var.16 = build_tuple(items=[Var($parfor__index_9.81, <string>:2), Var($parfor__index_10.83, <string>:3)])\n",
      "loop #1:\n",
      "  Has the following hoisted:\n",
      "    $48load_global.3 = global(range: <class 'range'>)\n",
      "    bool66 = global(bool: <class 'bool'>)\n",
      "  Failed to hoist the following:\n",
      "    dependency: $76binary_subscr.6 = getitem(value=_anime__indices_21, index=$parfor__index_17.87, fn=<built-in function getitem>)\n",
      "    dependency: $82binary_subscr.9 = getitem(value=_anime__indices_21, index=$j.25, fn=<built-in function getitem>)\n",
      "    dependency: $84build_tuple.10 = build_tuple(items=[Var($76binary_subscr.6, 1771784096.py:17), Var($82binary_subscr.9, 1771784096.py:17)])\n",
      "    dependency: $88binary_subscr.13 = getitem(value=cooccurrence__matrix, index=$84build_tuple.10, fn=<built-in function getitem>)\n",
      "    dependency: $94binary_subscr.16 = getitem(value=_ratings_1_22, index=$parfor__index_17.87, fn=<built-in function getitem>)\n",
      "    dependency: $100binary_subscr.19 = getitem(value=_ratings_1_22, index=$j.25, fn=<built-in function getitem>)\n",
      "    dependency: $102binary_multiply.20 = $94binary_subscr.16 * $100binary_subscr.19\n",
      "    dependency: $104inplace_add.21 = inplace_binop(fn=<built-in function iadd>, immutable_fn=<built-in function add>, lhs=$88binary_subscr.13, rhs=$102binary_multiply.20, static_lhs=Undefined, static_rhs=Undefined)\n",
      "    dependency: i = $parfor__index_17.87\n",
      "    not pure: $52call_function.5 = call $push_global_to_block.85(_rating__count__for__user_23, func=$push_global_to_block.85, args=[Var(_rating__count__for__user_23, 1771784096.py:9)], kws=(), vararg=None, varkwarg=None, target=None)\n",
      "    dependency: $54get_iter.6 = getiter(value=$52call_function.5)\n",
      "    dependency: $56for_iter.3 = iternext(value=$54get_iter.6)\n",
      "    dependency: $j.25 = pair_first(value=$56for_iter.3)\n",
      "    dependency: $56for_iter.5 = pair_second(value=$56for_iter.3)\n",
      "    dependency: $64compare_op.6 = $parfor__index_17.87 == $j.25\n",
      "    dependency: $66pred = call $push_global_to_block.86($64compare_op.6, func=$push_global_to_block.86, args=(Var($64compare_op.6, 1771784096.py:14),), kws=(), vararg=None, varkwarg=None, target=None)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from numba.np.ufunc import parallel\n",
    "\n",
    "@njit(parallel=True, nopython=True)\n",
    "def compute_cooccurrence_matrix(anime_count: int, ratings: list[(np.array, np.array)]) -> np.ndarray:\n",
    "    cooccurrence_matrix = np.zeros((anime_count, anime_count))\n",
    "\n",
    "    for anime_indices, ratings in ratings:\n",
    "        rating_count_for_user = len(anime_indices)\n",
    "\n",
    "        for i in prange(rating_count_for_user):\n",
    "            for j in range(rating_count_for_user):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                cooccurrence_matrix[anime_indices[i], anime_indices[j]] += ratings[i] * ratings[j]\n",
    "\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "print(f\"Users to process: {len(ratings)}\")\n",
    "cooccurrence_matrix = compute_cooccurrence_matrix(len(anime_title_by_id), ratings)\n",
    "\n",
    "compute_cooccurrence_matrix.parallel_diagnostics(level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323179.98000001477"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_matrix[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the co-occurrence matrix to compressed binary numpy file\n",
    "np.savez_compressed('/data/jx4237data/projects/web_page/sprout-main/work/data/cooccurrence_matrix.npz', cooccurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_matrix = np.load('/data/jx4237data/projects/web_page/sprout-main/work/data/cooccurrence_matrix.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40.0, 'Kill Me Baby: Butsuzou Kegatte Nise Halloween')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# def score_num_recommendations(num_recommendations):\n",
    "#     return math.sqrt(num_recommendations) / 2.\n",
    "\n",
    "def parse_extra(item):\n",
    "    related = json.loads(item.related_anime) \n",
    "    related = [(r['node']['id'], 0.2 if r['relation_type'] == 'character' else 40) for r in related]\n",
    "    # recommended = [(r['node']['id'], score_num_recommendations(r['num_recommendations'])) for r in json.loads(item.recommendations)]\n",
    "    # return [(anime_ix_by_id.get(id), score) for id, score in related + recommended if anime_ix_by_id.get(id) is not None]\n",
    "    return [(anime_ix_by_id.get(id), score) for id, score in related if anime_ix_by_id.get(id) is not None]\n",
    "\n",
    "\n",
    "related_by_anime_ix = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for item in all_animes:\n",
    "    anime_ix = anime_ix_by_id.get(item.id)\n",
    "    if anime_ix is None:\n",
    "        continue\n",
    "    related_for_item = parse_extra(item)\n",
    "    entry = related_by_anime_ix[anime_ix]\n",
    "    for ix, weight in related_for_item:\n",
    "        entry[ix] += weight\n",
    "        \n",
    "kill_me_baby_id = 11079\n",
    "kill_me_baby_ix = anime_ix_by_id[kill_me_baby_id]\n",
    "[(score, all_animes[ix].title) for ix, score in related_by_anime_ix[kill_me_baby_ix].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_matrix_wextra = cooccurrence_matrix.copy()\n",
    "\n",
    "for anime_0_ix in range(len(cooccurrence_matrix)):\n",
    "    related_for_anime0 = related_by_anime_ix[anime_0_ix]\n",
    "    max_weight = cooccurrence_matrix[anime_0_ix].max()\n",
    "\n",
    "    for ix, weight in related_for_anime0.items():\n",
    "        if ix == anime_0_ix:\n",
    "            continue\n",
    "        cooccurrence_matrix_wextra[anime_0_ix, ix] += weight * max_weight\n",
    "\n",
    "np.save('/data/jx4237data/projects/web_page/sprout-main/work/data/cooccurrence_matrix_wextra.npy', cooccurrence_matrix_wextra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323179.98000001477"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_matrix_wextra[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: 370659\n"
     ]
    }
   ],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "def get_topn(weights, topn):\n",
    "    return sorted(enumerate(weights), key=lambda x: x[1], reverse=True)[:topn]\n",
    "\n",
    "topn = 40\n",
    "\n",
    "def get_extra_topn(rating_count):\n",
    "    return int(max(math.pow(rating_count, 0.46) * 0.4 - 10, 0))\n",
    "\n",
    "for anime_0_ix in range(cooccurrence_matrix.shape[0]):\n",
    "    row = cooccurrence_matrix[anime_0_ix]\n",
    "    top_edges = get_topn(row, topn)\n",
    "    top_related_anime_ixs_set = set([ix for ix, weight in top_edges])\n",
    "\n",
    "    top_magnitude = 0 if len(top_edges) == 0 else top_edges[0][1]\n",
    "    if top_magnitude == 0:\n",
    "        continue\n",
    "    related_for_anime0 = related_by_anime_ix[anime_0_ix]\n",
    "\n",
    "    for related_anime_ix, related_weight in related_for_anime0.items():\n",
    "        if related_anime_ix in top_related_anime_ixs_set:\n",
    "            continue\n",
    "        top_edges.append((related_anime_ix, 0.))\n",
    "\n",
    "    for anime_1_ix, base_weight in top_edges:\n",
    "        extra_weight = related_for_anime0[anime_1_ix]\n",
    "        # This may need to be tuned\n",
    "        weight = base_weight + extra_weight * top_magnitude * 1.\n",
    "        graph.add_edge(anime_0_ix, anime_1_ix, weight=weight)\n",
    "\n",
    "print(\"Edges:\", graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csrgraph as cg\n",
    "import nodevectors\n",
    "\n",
    "cgraph = cg.csrgraph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0048\t:  12%|█▏        | 43/350 [00:40<04:47,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged! Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dimensions = 10\n",
    "order = 2\n",
    "algo = \"ggvec\"\n",
    "\n",
    "embedding_model = None\n",
    "if algo == \"ggvec\":\n",
    "    embedding_model = nodevectors.GGVec(n_components=dimensions, learning_rate=0.01, negative_ratio=0.6, verbose=True, order=order)\n",
    "elif algo == \"node2vec\":\n",
    "    embedding_model = nodevectors.Node2Vec(walklen=8, epochs=50, return_weight=2., neighbor_weight=1., n_components=dimensions, threads=14)\n",
    "elif algo == \"ProNE\":\n",
    "    embedding_model = nodevectors.ProNE(n_components=dimensions)\n",
    "\n",
    "embeddings = embedding_model.fit_transform(cgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key length: 9359; weight length: 9359; anime_count: 9432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/jx4237data/projects/web_page/sprout-main/work/data/embedding_ggvec_full_posonly_wextra_top40_10d_order2.w2v'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [int(n) for n in cgraph.nodes()]\n",
    "\n",
    "print(f\"key length: {len(keys)}; weight length: {len(embeddings)}; anime_count: {len(all_animes)}\")\n",
    "\n",
    "fname = f\"/data/jx4237data/projects/web_page/sprout-main/work/data/embedding_{algo}_full_posonly_wextra_top{topn}_{dimensions}d_order{order}.w2v\"\n",
    "with open(fname, 'wt') as f:\n",
    "    tab = ' '\n",
    "    nl = '\\n'\n",
    "    f.write(f\"{len(keys)}{tab}{dimensions}{nl}\")\n",
    "    for key, embedding in zip(keys, embeddings):\n",
    "        f.write(f\"{key}{tab}{tab.join(map(str, embedding))}{nl}\")\n",
    "fname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
